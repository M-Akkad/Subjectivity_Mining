{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d890959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade llama-cpp-python\n",
    "# !pip install openai\n",
    "# !pip install sse_starlette\n",
    "# !pip install starlette_context\n",
    "# !pip install pydantic_settings\n",
    "# !pip install \"fastapi[all]\"\n",
    "# !pip install pandas\n",
    "# !pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "118aa129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# from openai import OpenAI\n",
    "# # Point to the server\n",
    "# client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"cltl\")\n",
    "# # Sentences to classify\n",
    "# sentences = [\n",
    "# \"I hate you and I hope you fail.\",\n",
    "# \"What a beautiful day to go for a walk!\",\n",
    "# \"Your idea is stupid and nobody cares.\"\n",
    "# ]\n",
    "# # Build a single prompt\n",
    "# prompt = \"Classify each of the following sentences as 'hate' or 'non-hate':\\n\\n\"\n",
    "# for i, s in enumerate(sentences, 1):\n",
    "#     prompt += f\"{i}. {s}\\n\"\n",
    "    \n",
    "# prompt += \"\\nReturn the results in the format:\\n<number>. <label>\\n\"\n",
    "# # Make one request for all sentences\n",
    "# response = client.completions.create(\n",
    "#     model=\"local model\", # currently unused\n",
    "#     prompt=prompt,\n",
    "#     max_tokens=50,\n",
    "#     temperature=0,\n",
    "#     stop=[\"Classify\", \"\\n\\n\"]\n",
    "#     )\n",
    "# # Print the raw model output\n",
    "# print(response.choices[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43cebed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3307549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "\n",
    "# Initialize OpenAI client pointing to local LLaMA server\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"cltl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55782f5",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eda32476",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r'Subjectivity_mining_assignment_4_5_data\\olid-train-small.csv')\n",
    "test_df = pd.read_csv(r'Subjectivity_mining_assignment_4_5_data\\OLID-test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d215a76c",
   "metadata": {},
   "source": [
    "### Helper methodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "019bcd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(prompt):\n",
    "    \"\"\"Get prediction from model\"\"\"\n",
    "    try:\n",
    "        response = client.completions.create(\n",
    "            model=\"local model\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=10,\n",
    "            temperature=0,\n",
    "            stop=[\"\\n\", \"Text:\", \"Classification:\"]\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "    except:\n",
    "        return \"non-offensive\"\n",
    "\n",
    "def parse_response(response, use_toxic_labels=False):\n",
    "    \"\"\"Convert model response to 0 or 1\"\"\"\n",
    "    response_lower = response.lower().strip()\n",
    "    \n",
    "    if use_toxic_labels:\n",
    "        if 'toxic' in response_lower and 'non-toxic' not in response_lower:\n",
    "            return 1\n",
    "        return 0\n",
    "    else:\n",
    "        if 'offensive' in response_lower and 'non-offensive' not in response_lower:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "def get_random_examples(n_examples):\n",
    "    \"\"\"Get random balanced examples from train set\"\"\"\n",
    "    n_per_class = n_examples // 2\n",
    "    \n",
    "    offensive = train_df[train_df['label'] == 1].sample(n=n_per_class, random_state=42)\n",
    "    non_offensive = train_df[train_df['label'] == 0].sample(n=n_per_class, random_state=42)\n",
    "    \n",
    "    examples = []\n",
    "    for _, row in offensive.iterrows():\n",
    "        examples.append((row['text'], 1))\n",
    "    for _, row in non_offensive.iterrows():\n",
    "        examples.append((row['text'], 0))\n",
    "    \n",
    "    random.shuffle(examples)\n",
    "    return examples\n",
    "\n",
    "def evaluate_prompts(test_df, predictions):\n",
    "    \"\"\"Calculate metrics\"\"\"\n",
    "    true_labels = test_df['label'].tolist()\n",
    "    \n",
    "    report = classification_report(true_labels, predictions, \n",
    "                                   target_names=['Non-Offensive', 'Offensive'],\n",
    "                                   output_dict=True)\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    \n",
    "    return report, cm\n",
    "\n",
    "def print_results(name, report, cm):\n",
    "    \"\"\"Print results\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Strategy: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Macro F1: {report['macro avg']['f1-score']:.3f}\")\n",
    "    print(f\"Offensive F1: {report['Offensive']['f1-score']:.3f}\")\n",
    "    print(f\"Non-Offensive F1: {report['Non-Offensive']['f1-score']:.3f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"                 Predicted\")\n",
    "    print(f\"               Non-OFF  OFF\")\n",
    "    print(f\"Actual Non-OFF  {cm[0][0]:>6}  {cm[0][1]:>6}\")\n",
    "    print(f\"       OFF      {cm[1][0]:>6}  {cm[1][1]:>6}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21bee5",
   "metadata": {},
   "source": [
    "### VANILLA ZERO-SHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6e4183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXPERIMENT 1: VANILLA ZERO-SHOT\n",
      "============================================================\n",
      "Processed 0/860\n",
      "Processed 100/860\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 1: VANILLA ZERO-SHOT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "predictions_vanilla = []\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    prompt = f\"\"\"Classify the following text as 'offensive' or 'non-offensive':\n",
    "\n",
    "Text: {row['text']}\n",
    "\n",
    "Classification:\"\"\"\n",
    "    \n",
    "    response = get_prediction(prompt)\n",
    "    pred = parse_response(response)\n",
    "    predictions_vanilla.append(pred)\n",
    "    \n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processed {idx}/{len(test_df)}\")\n",
    "\n",
    "report_vanilla, cm_vanilla = evaluate_prompts(test_df, predictions_vanilla)\n",
    "print_results(\"Vanilla Zero-Shot\", report_vanilla, cm_vanilla)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c20033",
   "metadata": {},
   "source": [
    "### DEFINITION-AUGMENTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b7f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 2: DEFINITION-AUGMENTED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "predictions_def = []\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    prompt = f\"\"\"Definitions:\n",
    "- Offensive: Contains insults, threats, profanity, or targets individuals/groups based on identity.\n",
    "- Non-offensive: Does not contain offensive language or harmful content.\n",
    "\n",
    "Classify the following text as 'offensive' or 'non-offensive':\n",
    "\n",
    "Text: {row['text']}\n",
    "\n",
    "Classification:\"\"\"\n",
    "    \n",
    "    response = get_prediction(prompt)\n",
    "    pred = parse_response(response)\n",
    "    predictions_def.append(pred)\n",
    "    \n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processed {idx}/{len(test_df)}\")\n",
    "\n",
    "report_def, cm_def = evaluate_prompts(test_df, predictions_def)\n",
    "print_results(\"Definition-Augmented\", report_def, cm_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c456a97f",
   "metadata": {},
   "source": [
    "### ALTERNATIVE DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20864d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 3: ALTERNATIVE DEFINITION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "predictions_alt_def = []\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    prompt = f\"\"\"Definitions:\n",
    "- Offensive: Text that could cause harm, distress, or is intended to demean, dehumanize, or attack individuals or groups.\n",
    "- Non-offensive: Text that is respectful, neutral, or positive in nature.\n",
    "\n",
    "Classify the following text as 'offensive' or 'non-offensive':\n",
    "\n",
    "Text: {row['text']}\n",
    "\n",
    "Classification:\"\"\"\n",
    "    \n",
    "    response = get_prediction(prompt)\n",
    "    pred = parse_response(response)\n",
    "    predictions_alt_def.append(pred)\n",
    "    \n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processed {idx}/{len(test_df)}\")\n",
    "\n",
    "report_alt_def, cm_alt_def = evaluate_prompts(test_df, predictions_alt_def)\n",
    "print_results(\"Alternative Definition (Harm-based)\", report_alt_def, cm_alt_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e3db35",
   "metadata": {},
   "source": [
    "### ALTERNATIVE LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd382cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 4: ALTERNATIVE LABELS (toxic/non-toxic)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "predictions_toxic = []\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    prompt = f\"\"\"Definitions:\n",
    "- Toxic: Contains harmful, abusive, or hateful language.\n",
    "- Non-toxic: Does not contain harmful or abusive language.\n",
    "\n",
    "Classify the following text as 'toxic' or 'non-toxic':\n",
    "\n",
    "Text: {row['text']}\n",
    "\n",
    "Classification:\"\"\"\n",
    "    \n",
    "    response = get_prediction(prompt)\n",
    "    pred = parse_response(response, use_toxic_labels=True)\n",
    "    predictions_toxic.append(pred)\n",
    "    \n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processed {idx}/{len(test_df)}\")\n",
    "\n",
    "report_toxic, cm_toxic = evaluate_prompts(test_df, predictions_toxic)\n",
    "print_results(\"Alternative Labels (Toxic)\", report_toxic, cm_toxic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ced566",
   "metadata": {},
   "source": [
    "### FEW-SHOT RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68791173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5a: 2-shot\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 5a: FEW-SHOT RANDOM (2 examples)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "examples_2 = get_random_examples(2)\n",
    "predictions_2shot = []\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    prompt = \"Classify texts as 'offensive' or 'non-offensive'.\\n\\nExamples:\\n\\n\"\n",
    "    \n",
    "    for ex_text, ex_label in examples_2:\n",
    "        label_str = \"offensive\" if ex_label == 1 else \"non-offensive\"\n",
    "        prompt += f\"Text: {ex_text}\\nClassification: {label_str}\\n\\n\"\n",
    "    \n",
    "    prompt += f\"Text: {row['text']}\\nClassification:\"\n",
    "    \n",
    "    response = get_prediction(prompt)\n",
    "    pred = parse_response(response)\n",
    "    predictions_2shot.append(pred)\n",
    "    \n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processed {idx}/{len(test_df)}\")\n",
    "\n",
    "report_2shot, cm_2shot = evaluate_prompts(test_df, predictions_2shot)\n",
    "print_results(\"Few-Shot Random (2 examples)\", report_2shot, cm_2shot)\n",
    "\n",
    "# 5b: 4-shot\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 5b: FEW-SHOT RANDOM (4 examples)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "examples_4 = get_random_examples(4)\n",
    "predictions_4shot = []\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    prompt = \"Classify texts as 'offensive' or 'non-offensive'.\\n\\nExamples:\\n\\n\"\n",
    "    \n",
    "    for ex_text, ex_label in examples_4:\n",
    "        label_str = \"offensive\" if ex_label == 1 else \"non-offensive\"\n",
    "        prompt += f\"Text: {ex_text}\\nClassification: {label_str}\\n\\n\"\n",
    "    \n",
    "    prompt += f\"Text: {row['text']}\\nClassification:\"\n",
    "    \n",
    "    response = get_prediction(prompt)\n",
    "    pred = parse_response(response)\n",
    "    predictions_4shot.append(pred)\n",
    "    \n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processed {idx}/{len(test_df)}\")\n",
    "\n",
    "report_4shot, cm_4shot = evaluate_prompts(test_df, predictions_4shot)\n",
    "print_results(\"Few-Shot Random (4 examples)\", report_4shot, cm_4shot)\n",
    "\n",
    "# 5c: 6-shot\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 5c: FEW-SHOT RANDOM (6 examples)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "examples_6 = get_random_examples(6)\n",
    "predictions_6shot = []\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    prompt = \"Classify texts as 'offensive' or 'non-offensive'.\\n\\nExamples:\\n\\n\"\n",
    "    \n",
    "    for ex_text, ex_label in examples_6:\n",
    "        label_str = \"offensive\" if ex_label == 1 else \"non-offensive\"\n",
    "        prompt += f\"Text: {ex_text}\\nClassification: {label_str}\\n\\n\"\n",
    "    \n",
    "    prompt += f\"Text: {row['text']}\\nClassification:\"\n",
    "    \n",
    "    response = get_prediction(prompt)\n",
    "    pred = parse_response(response)\n",
    "    predictions_6shot.append(pred)\n",
    "    \n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processed {idx}/{len(test_df)}\")\n",
    "\n",
    "report_6shot, cm_6shot = evaluate_prompts(test_df, predictions_6shot)\n",
    "print_results(\"Few-Shot Random (6 examples)\", report_6shot, cm_6shot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760ca8a3",
   "metadata": {},
   "source": [
    "### FEW-SHOT RELEVANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539274da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 6: FEW-SHOT RELEVANT EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Manually selected clear examples\n",
    "relevant_examples = [\n",
    "    (\"You are a worthless piece of trash\", 1),\n",
    "    (\"This is a wonderful day!\", 0),\n",
    "    (\"Go kill yourself idiot\", 1),\n",
    "    (\"I disagree with your opinion but respect it\", 0)\n",
    "]\n",
    "\n",
    "predictions_relevant = []\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    prompt = \"\"\"Classify texts as 'offensive' or 'non-offensive'.\n",
    "\n",
    "Offensive texts contain insults, threats, or derogatory language.\n",
    "Non-offensive texts are neutral or positive.\n",
    "\n",
    "Examples:\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    for ex_text, ex_label in relevant_examples:\n",
    "        label_str = \"offensive\" if ex_label == 1 else \"non-offensive\"\n",
    "        prompt += f\"Text: {ex_text}\\nClassification: {label_str}\\n\\n\"\n",
    "    \n",
    "    prompt += f\"Text: {row['text']}\\nClassification:\"\n",
    "    \n",
    "    response = get_prediction(prompt)\n",
    "    pred = parse_response(response)\n",
    "    predictions_relevant.append(pred)\n",
    "    \n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processed {idx}/{len(test_df)}\")\n",
    "\n",
    "report_relevant, cm_relevant = evaluate_prompts(test_df, predictions_relevant)\n",
    "print_results(\"Few-Shot Relevant Examples\", report_relevant, cm_relevant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc8c909",
   "metadata": {},
   "source": [
    "### SUMMARY TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf5270",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Strategy':<40} {'Macro F1':<12} {'OFF F1':<12} {'NON-OFF F1':<12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "all_results = [\n",
    "    (\"Vanilla Zero-Shot\", report_vanilla),\n",
    "    (\"Definition-Augmented\", report_def),\n",
    "    (\"Alternative Definition\", report_alt_def),\n",
    "    (\"Alternative Labels (Toxic)\", report_toxic),\n",
    "    (\"Few-Shot Random (2 examples)\", report_2shot),\n",
    "    (\"Few-Shot Random (4 examples)\", report_4shot),\n",
    "    (\"Few-Shot Random (6 examples)\", report_6shot),\n",
    "    (\"Few-Shot Relevant\", report_relevant)\n",
    "]\n",
    "\n",
    "for name, report in all_results:\n",
    "    macro_f1 = report['macro avg']['f1-score']\n",
    "    off_f1 = report['Offensive']['f1-score']\n",
    "    non_off_f1 = report['Non-Offensive']['f1-score']\n",
    "    print(f\"{name:<40} {macro_f1:<12.3f} {off_f1:<12.3f} {non_off_f1:<12.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17654241",
   "metadata": {},
   "source": [
    "### SAVE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86731837",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {\n",
    "    'vanilla': {'report': report_vanilla, 'confusion_matrix': cm_vanilla.tolist()},\n",
    "    'definition': {'report': report_def, 'confusion_matrix': cm_def.tolist()},\n",
    "    'alt_definition': {'report': report_alt_def, 'confusion_matrix': cm_alt_def.tolist()},\n",
    "    'toxic_labels': {'report': report_toxic, 'confusion_matrix': cm_toxic.tolist()},\n",
    "    'few_shot_2': {'report': report_2shot, 'confusion_matrix': cm_2shot.tolist()},\n",
    "    'few_shot_4': {'report': report_4shot, 'confusion_matrix': cm_4shot.tolist()},\n",
    "    'few_shot_6': {'report': report_6shot, 'confusion_matrix': cm_6shot.tolist()},\n",
    "    'few_shot_relevant': {'report': report_relevant, 'confusion_matrix': cm_relevant.tolist()}\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open('results_summary.json', 'w') as f:\n",
    "    json.dump(results_dict, f, indent=2)\n",
    "\n",
    "print(\"\\n\\nResults saved to results_summary.json\")\n",
    "print(\"Experiment completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1295593",
   "metadata": {},
   "source": [
    "### DETAILED ANALYSIS OF TWO STRATEGIES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12c4dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED ANALYSIS OF SELECTED STRATEGIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Strategy 1: Few-Shot Random (4 examples)\n",
    "print(\"\\nStrategy 1: Few-Shot Random (4 examples)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Precision (Offensive): {report_4shot['Offensive']['precision']:.3f}\")\n",
    "print(f\"Recall (Offensive): {report_4shot['Offensive']['recall']:.3f}\")\n",
    "print(f\"F1-Score (Offensive): {report_4shot['Offensive']['f1-score']:.3f}\")\n",
    "print(f\"\\nPrecision (Non-Offensive): {report_4shot['Non-Offensive']['precision']:.3f}\")\n",
    "print(f\"Recall (Non-Offensive): {report_4shot['Non-Offensive']['recall']:.3f}\")\n",
    "print(f\"F1-Score (Non-Offensive): {report_4shot['Non-Offensive']['f1-score']:.3f}\")\n",
    "\n",
    "# Strategy 2: Few-Shot Relevant\n",
    "print(\"\\nStrategy 2: Few-Shot Relevant Examples\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Precision (Offensive): {report_relevant['Offensive']['precision']:.3f}\")\n",
    "print(f\"Recall (Offensive): {report_relevant['Offensive']['recall']:.3f}\")\n",
    "print(f\"F1-Score (Offensive): {report_relevant['Offensive']['f1-score']:.3f}\")\n",
    "print(f\"\\nPrecision (Non-Offensive): {report_relevant['Non-Offensive']['precision']:.3f}\")\n",
    "print(f\"Recall (Non-Offensive): {report_relevant['Non-Offensive']['recall']:.3f}\")\n",
    "print(f\"F1-Score (Non-Offensive): {report_relevant['Non-Offensive']['f1-score']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL EXPERIMENTS COMPLETED!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
